<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Exploit Exercises]]></title>
  <link href="https://exploit-exercises.com/atom.xml" rel="self"/>
  <link href="https://exploit-exercises.com/"/>
  <updated>2014-10-23T10:42:38+00:00</updated>
  <id>https://exploit-exercises.com/</id>
  <author>
    <name><![CDATA[Exploit Exercises]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Week Before Ruxcon, Disk Failure Strikes]]></title>
    <link href="https://exploit-exercises.com/blog/2014/10/22/a-week-before-ruxcon/"/>
    <updated>2014-10-22T08:50:41+00:00</updated>
    <id>https://exploit-exercises.com/blog/2014/10/22/a-week-before-ruxcon</id>
    <content type="html"><![CDATA[<p>A week before Ruxcon, I attempted to log into the virtual machines that we had
setup. First one worked fine, the other two were hanging during the SSH
process.</p>

<p>Out of curiousity, I did dmesg on the first one to see if that could give me
any indications as to why the others were not responding.</p>

<p>And, it did. Unfortunately. dmesg indicated that there was hard drive errors.
Due to not having correct backups in place, we had to attempt to recover the
data from the hard drive. Enter ddrescue.</p>

<p>After the initial 40 gigabytes or so copying from the hard drive, the errors
would show up again, and the machine had to be rebooted. Rinse, repeat, same
results. Around that time, another Ruxcon staff member rang me, and suggested
that I try the ddrescue reverse copy mode, and see if that works.</p>

<p>This allowed an extra 140 gigabytes to be recovered, with a total of 20
gigabytes give or take not able to be rescued. So, initial data recover done,
time to assess the further damage. After using vmfs-fuse to mount the recovered
data, I tried to copy the VMDK&rsquo;s off the disk. All 3 gave IO errors at some
point (due to VMFS corruption).</p>

<p>So I ddrescue&rsquo;d the VMDK&rsquo;s to get the data off those, used qemu-nbd to mount
them. Initially I didn&rsquo;t want to fsck anything to see what was available, but
everything was a mess. After trying to use fsck on the various VMDK&rsquo;s, I got
mixed results.</p>

<p>One machine looked completely fine, except nothing where the levels should have
been, and nothing in /lost+found. One machine had <strong>everything</strong> in
/lost+found, in a variety of mixed places.</p>

<p>The last machine was mostly alright. My first impression was that everything
was fine. I copied data off, and started to rebuild stuff, and&hellip; errors. Turns
out there was a lot of files with just NULL&rsquo;s in them, and some directories
were missing. I was able to piece together a mostly up to date version from
/lost+found, luckily.</p>

<p>All that effort could have been avoided had I used appropriate backups, and
deployed directly to the VM&rsquo;s as opposed to working on them.</p>
]]></content>
  </entry>
  
</feed>
